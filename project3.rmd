---
title: "Project 3"
author: "Rachel Fellman & Sabrina Dahl"
params: 
  Ed: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
```

# Introduction

# Data

We will start by reading in the data with the `read_csv` function with a relative path. 

```{r, message=FALSE}
diabetes <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")
```

Next we will combine the 1 and 2 groups in the `Education` variable into a new group. We will do this using the `mutate` and `ifelse` functions. The new value for the 1 and 2 groups will just be 1, so `Education` can equal 1, 3, 4, 5, or 6.

```{r}
diabetes.1 <- diabetes %>% 
  mutate(Education = ifelse(Education < 3, 1, Education))
```

# Summarizations

We will start by doing some exploratory data analysis on the full data set. 

Since we will be creating models for different education levels, we will split up the data into smaller data set for each of the 5 education levels to do EDA. We will do this using our parameters
```{r}
#create subsetted data set for each level of education using the params.
diabetes.ed <- diabetes.1 %>% 
  filter(Education == params$Ed)
```

We will start by looking at a scatter plot of the proportion of people with diabetes across BMI levels. First we will create the proportion variable and then we will use `ggplot` with `geom_point` to create the graph.
```{r}
# group by BMI to create graph of proportion of people with diabetes at each BMI.
bmi.sum <- diabetes.ed %>% 
  group_by(BMI) %>% 
  summarise(proportion.diabetes = mean(Diabetes_binary), n = n())
# create plot
ggplot(bmi.sum, aes(x = BMI, y = proportion.diabetes, size = n)) +
  geom_point(stat = "identity")+ 
#add title
  labs(title = "Proportion of People with Diabetes at each BMI for Education = 1")
```
When looking at this graph is is helpful to look for trends. You can examine the graph to see if there is a positive or negative correlation between the BMI and the proportion of people with diabetes. It can also be good to look for outliers because these may affect our results in unexpected ways. Another aspect to pay attention to would be if there is more data at certain levels of BMI and how this might affect our ability to predict.


Next we will look at a Graph of proportion of Diabetes for different ages. First we will create the proportion variable and then we will use `ggplot` with `geom_point` to create the graph.
```{r}
# group by BMI to create graph of proportion of people with diabetes at each age.
age.sum <- diabetes.ed %>% 
  group_by(Age) %>% 
  summarise(proportion.diabetes = mean(Diabetes_binary))
# create plot
ggplot(age.sum, aes(x = Age, y = proportion.diabetes)) +
  geom_point(stat = "identity")+ 
#add title
  labs(title = "Proportion of People with Diabetes at each age")
```
When examining this graph, it is helpful to look for trends in the data. For example, do higher ages have higher proportions of diabetes or lower proportions? You can also think about what kind of trend line would best describe the data. Is the relationship linear, parabolic, etc. ? Keep in mind that the age is split into categories with 1 being 18 - 24, and 13 being 80 and older. The age levels are done in 5 year increments. 

Next we will create a graph of diabetes proportion vs income. First we will create the proportion variable and then we will use `ggplot` with `geom_point` to create the graph. The inclome levels are as follows: 1 = < $10,000, 5 = 
```{r}
# group by income to create graph of proportion of people with diabetes at each income for education 1.
income.sum <- diabetes.ed %>% 
  group_by(Income) %>% 
  summarise(proportion.diabetes = mean(Diabetes_binary), n= n())
# create plot
ggplot(income.sum, aes(x = Income, y = proportion.diabetes, size = n)) +
  geom_point(stat = "identity")+ 
#add title
  labs(title = "Proportion of People with Diabetes at each income level")
```
This graph shows us the proportion of people with diabetes at each income level. Level 1 is less than 10,000 dollars and it goes all the way up to level 8 which is greater than or equal to 75,000 dollars. It is helpful to see if the graph shows any positive or negative correlations between the income and the proportion of people with diabetes. Since this graph also includes larger points when there is more data, you can see if there are more data at lower or higher incomes.

Next we will create a table to examine some of our binary variables. We will create a table for the number of people with diabetes by if they have any health care or not. This will be done via the `table` function. I will also add a row to the table for the proportion of people with diabetes to help us better understand the data.

```{r}
tab<- table(diabetes.ed$Diabetes_binary, diabetes.ed1$AnyHealthcare)
rbind(tab, apply(tab, MARGIN = 2, FUN = function(x){round(x[2] / sum(x), 3)}))
```
In this table we have the columns giving the count of people who have health care (0 = no, 1 = yes), and the rows providing the count of people with diabetes( 0 = no, 1 = yes). The last row of the table gives the proportion of people with diabetes for each column. It is helpful to look at these proportions and see if there are greater percents of people with diabetes for those who have healthcare vs those who don't. 

We will also examine the average and standard deviation for mental health levels. The mental health level tells us the number of days during the past month that the person felt their mental health was not good from a scale of 1 to 30 days. We will group by diabetes, since this is the thing we are wanting to predict, (0 = no, 1 = yes) as well as by sex (0 = female, and 1 = male).
```{r, message=FALSE}
diabetes.ed %>% 
  group_by(Diabetes_binary, Sex) %>% 
  summarise(avg = mean(MentHlth), sd = sd(MentHlth))
```

In this table we can look at the differences between the averages of bad mental health days for men and women who have diabetes and who do not. Here it can be helpful to examine if there is a higher average for those with diabetes than for those without. You can also look for any differences between the averages of those with diabetes who are men and those with diabetes who are women. 


# Modeling

Before doing any modeling we will split the data into a training and test set using the `createDataPartition` function from the `caret` package. The test/train data will be split with a 70/30 ratio.

```{r}
#make things reproducible with set.seed (I have no idea what number we are supposed to put here because I don't really know what this function does)
set.seed(90)
#create index
train.index <- createDataPartition(diabetes.ed$Diabetes_binary, p = .7, list = FALSE)
#create train and test sets
diabetes.train <- diabetes.ed[train.index, ]
diabetes.test <- diabetes.ed[-train.index, ]
```

## Log Loss

Log loss is a measurement we can use to assess a model in a classification problem. A classification problem is one where we have a categorical or binary response and we want to either predict class membership or the probability of membership with our models. Log loss gives us a measurement of how close the predicted probability of being in a class is to the actual value (0 or 1 in the case of binary classification which we have in this project). A higher log loss value indicates that the predicted probability is farther from the actual value, and a perfect model would have a log loss of 0. Although accuracy is a commonly used method for measuring the performance of our models, log loss could be superior. Accuracy is the ratio of correct predictions to total predictions. This measurement can fall short in two cases. First, if there are more than two classes, the model may ignore some classes. In this case, the accuracy measurement doesn't give the individual accuracy for each class which is likely the information we actually want. Second, the accuracy metric works poorly for imbalanced data. If we had a binary classification data set with a ratio of 95:5 for the 2 classes, then the model may automatically assign everything to the first class. This would give us a accuracy rate of 95% that tells us little about the model. Log loss can combat these problems. Log loss takes into account how close a prediction is to the actual value, that is to say it considers the certainty of a prediction and penalizes predictions that are less certain. If the prediction was close to the correct class there will be lower loss. It provides a continuous measure to the model's performance as opposed to a binary measure like accuracy. 

## Logistic Regression



# Model Selection

